"""Point d'entr√©e principal du syst√®me de tags √©mergents (v2 optimis√©)."""

import sys
import io

# Force UTF-8 encoding for Windows console
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

from pathlib import Path
from typing import Optional
import click
import time

from .parsers import NoteParser
from .embeddings import Embedder
from .analysis.similarity_v2 import SimilarityEngineV2, SimilarityConfigV2
from .analysis.entity_detector_v2 import EntityDetectorV2, aggregate_entities_v2, EntityType
from .analysis.entity_classifier import ReferenceDatabase
from .clustering.detector_v2 import ClusterDetectorV2
from .tags import TagHealthAnalyzer, TagGenerator, TagMatcher, FeedbackIntegrator, RedundancyDetector
from .tags.conventions import TagFamily, classify_tag, suggest_tag_format, get_tag_family_label
from .database import Repository
from .output import SuggestionGenerator


@click.command()
@click.option(
    "--vault-path",
    required=True,
    type=click.Path(exists=True),
    help="Chemin vers le vault Obsidian",
)
@click.option(
    "--output",
    required=True,
    type=click.Path(),
    help="Chemin du fichier JSON de sortie",
)
@click.option(
    "--db-path",
    default="backend/data/tags.db",
    type=click.Path(),
    help="Chemin de la base de donn√©es SQLite",
)
@click.option(
    "--decisions",
    type=click.Path(),
    help="Chemin du fichier de d√©cisions (feedback loop)",
)
@click.option(
    "--min-similarity",
    default=0.65,
    type=float,
    help="Seuil minimum de similarit√© (0-1)",
)
@click.option(
    "--min-cluster-size",
    default=3,
    type=int,
    help="Taille minimum d'un cluster",
)
@click.option(
    "--verbose",
    is_flag=True,
    help="Mode verbeux",
)
def cli(
    vault_path: str,
    output: str,
    db_path: str,
    decisions: Optional[str],
    min_similarity: float,
    min_cluster_size: int,
    verbose: bool,
):
    """Analyse un vault Obsidian et g√©n√®re des suggestions de tags."""
    analyze_vault(
        vault_path=vault_path,
        output_path=output,
        db_path=db_path,
        decisions_path=decisions,
        min_similarity=min_similarity,
        min_cluster_size=min_cluster_size,
        verbose=verbose,
    )


def analyze_vault(
    vault_path: str,
    output_path: str,
    db_path: str = "backend/data/tags.db",
    decisions_path: Optional[str] = None,
    min_similarity: float = 0.65,
    min_cluster_size: int = 3,
    verbose: bool = True,
) -> dict:
    """Fonction principale d'analyse du vault (v2 optimis√©e).

    Supporte efficacement jusqu'√† 40K+ notes.
    """
    start_time = time.time()
    vault_path = Path(vault_path)
    db_path = Path(db_path)

    # Cr√©e le r√©pertoire de la DB si n√©cessaire
    db_path.parent.mkdir(parents=True, exist_ok=True)

    if verbose:
        print(f"üè∑Ô∏è  Analyse du vault: {vault_path}")
        print(f"üì¶ Base de donn√©es: {db_path}")
        print("=" * 50)

    # Initialise le repository
    repository = Repository(str(db_path))

    # 1. Parse toutes les notes
    step_start = time.time()
    if verbose:
        print("\n1. Parsing des notes...")

    parser = NoteParser(vault_path)
    notes = parser.parse_vault()

    if verbose:
        print(f"   ‚úì {len(notes)} notes trouv√©es ({time.time() - step_start:.1f}s)")

    if not notes:
        print("‚ö†Ô∏è  Aucune note trouv√©e. Arr√™t.")
        return {"status": "empty", "notes": 0}

    # Nettoie les notes supprim√©es de la DB
    current_paths = [n.path for n in notes]
    deleted = repository.delete_notes_not_in(current_paths)
    if verbose and deleted > 0:
        print(f"   üóëÔ∏è  {deleted} notes supprim√©es de la DB")

    # 2. Int√®gre le feedback si disponible
    feedback_stats = None
    if decisions_path and Path(decisions_path).exists():
        step_start = time.time()
        if verbose:
            print("\n2. Int√©gration du feedback...")

        feedback_integrator = FeedbackIntegrator(repository)
        decisions_data = feedback_integrator.load_decisions_from_file(decisions_path)

        if decisions_data:
            integrated = feedback_integrator.integrate_decisions(decisions_data)
            feedback_stats = feedback_integrator.get_feedback_stats()
            if verbose:
                print(f"   ‚úì {integrated} d√©cisions int√©gr√©es ({time.time() - step_start:.1f}s)")
                print(f"   üìä Taux d'acceptation: {int(feedback_stats.acceptance_rate * 100)}%")
    else:
        if verbose:
            print("\n2. Pas de feedback √† int√©grer")

    # 3. Initialise l'embedder et le moteur de similarit√© v2
    step_start = time.time()
    if verbose:
        print("\n3. Initialisation du moteur de similarit√©...")

    embedder = Embedder(repository=repository, use_cache=True)

    config = SimilarityConfigV2(
        min_similarity=min_similarity,
        min_cluster_size=min_cluster_size,
        batch_size=100,
    )
    similarity_engine = SimilarityEngineV2(embedder, config)

    # 4. Indexation des notes (embeddings + index vectoriel)
    step_start = time.time()
    if verbose:
        print("\n4. Indexation et embeddings...")

    similarity_engine.index_notes(notes, show_progress=verbose)

    if verbose:
        print(f"   ‚úì Indexation termin√©e ({time.time() - step_start:.1f}s)")

    # 5. D√©tection des clusters
    step_start = time.time()
    if verbose:
        print("\n5. D√©tection des clusters...")

    cluster_detector = ClusterDetectorV2(
        similarity_engine,
        min_cluster_size=min_cluster_size,
        min_similarity=min_similarity,
    )
    clusters = cluster_detector.detect_clusters(show_progress=verbose)

    if verbose:
        quality = cluster_detector.evaluate_clustering_quality(clusters)
        print(f"   ‚úì {len(clusters)} clusters ({time.time() - step_start:.1f}s)")
        print(f"   üìä Couverture: {quality['coverage']:.1%}, Coh√©rence moy: {quality['avg_coherence']:.2f}")

    # 6. Collecte des tags existants
    step_start = time.time()
    existing_tags = set()
    for note in notes:
        existing_tags.update(note.tags)
    existing_tags = list(existing_tags)

    if verbose:
        print(f"\n6. {len(existing_tags)} tags existants trouv√©s")

    # Met √† jour les tags dans la DB (par batch pour performance)
    for tag in existing_tags:
        usage = sum(1 for n in notes if tag in n.tags)
        repository.upsert_tag(name=tag, usage_count=usage)

    # 7. Analyse de sant√© des tags (optimis√©e)
    step_start = time.time()
    if verbose:
        print("\n7. Analyse de sant√© des tags...")

    notes_dict = {n.path: n for n in notes}
    health_analyzer = TagHealthAnalyzer(notes_dict, embedder, repository)

    # Limite le nombre d'alertes pour performance
    health_alerts = health_analyzer.get_health_alerts(max_alerts=500)

    if verbose:
        vault_health = health_analyzer.compute_vault_health_score()
        print(f"   ‚úì Score de sant√©: {vault_health:.0%} ({time.time() - step_start:.1f}s)")
        print(f"   ‚ö†Ô∏è  {len(health_alerts)} alertes g√©n√©r√©es")

    # 8. G√©n√©ration de nouveaux tags (bas√©e sur clusters + entit√©s)
    step_start = time.time()
    if verbose:
        print("\n8. G√©n√©ration de suggestions de nouveaux tags...")

    # Adapte le TagGenerator pour utiliser les clusters v2
    tag_generator = TagGeneratorV2(
        clusters=clusters,
        similarity_engine=similarity_engine,
        existing_tags=existing_tags,
        repository=repository,
        notes=notes,  # Ajout pour d√©tection d'entit√©s
    )
    new_tag_suggestions = tag_generator.generate_suggestions(max_suggestions=50)

    if verbose:
        cluster_count = sum(1 for s in new_tag_suggestions if s.get("source") == "cluster")
        entity_count = sum(1 for s in new_tag_suggestions if s.get("source") == "entity")
        print(f"   ‚úì {len(new_tag_suggestions)} nouveaux tags sugg√©r√©s ({time.time() - step_start:.1f}s)")
        print(f"   üìä {cluster_count} depuis clusters, {entity_count} depuis entit√©s")

    # 9. Matching de tags existants
    step_start = time.time()
    if verbose:
        print("\n9. Recherche d'attributions de tags existants...")

    tag_matcher = TagMatcherV2(
        notes_dict=notes_dict,
        similarity_engine=similarity_engine,
        existing_tags=existing_tags,
        repository=repository,
    )
    tag_assignments = tag_matcher.find_suggestions(max_suggestions=100)

    if verbose:
        print(f"   ‚úì {len(tag_assignments)} attributions sugg√©r√©es ({time.time() - step_start:.1f}s)")

    # 10. D√©tection des tags redondants (doublons s√©mantiques)
    step_start = time.time()
    if verbose:
        print("\n10. D√©tection des tags redondants...")

    # Calcule l'usage de chaque tag
    tag_usage = {}
    for tag in existing_tags:
        tag_usage[tag] = sum(1 for n in notes if tag in n.tags)

    redundancy_detector = RedundancyDetector(
        embedder=embedder,
        tag_usage=tag_usage,
        repository=repository,
    )
    redundant_groups = redundancy_detector.detect_redundant_groups(max_groups=50)

    if verbose:
        print(f"   ‚úì {len(redundant_groups)} groupes de doublons d√©tect√©s ({time.time() - step_start:.1f}s)")

    # 11. G√©n√®re le fichier de sortie
    if verbose:
        print(f"\n11. G√©n√©ration du fichier de sortie...")

    # Convertit les clusters v2 au format attendu par SuggestionGenerator
    clusters_for_output = [
        {
            "id": f"cl_{c.id:03d}",
            "notes": c.notes,
            "coherence": c.coherence,
            "centroid_terms": c.centroid_terms,
            "suggested_tags": c.suggested_tags,
        }
        for c in clusters
    ]

    output_generator = SuggestionGenerator(
        new_tags=new_tag_suggestions,
        tag_assignments=tag_assignments,
        health_alerts=health_alerts,
        clusters=clusters_for_output,
        total_notes=len(notes),
        total_tags=len(existing_tags),
        health_analyzer=health_analyzer,
        feedback_stats=feedback_stats,
        redundant_tags=redundant_groups,
    )

    # Cr√©e le r√©pertoire de sortie si n√©cessaire
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)

    output_generator.save_to_file(output_path)

    # Ferme la connexion DB
    repository.close()

    # R√©sum√©
    total_time = time.time() - start_time
    stats = {
        "status": "success",
        "notes_analyzed": len(notes),
        "existing_tags": len(existing_tags),
        "clusters_detected": len(clusters),
        "new_tags_suggested": len(new_tag_suggestions),
        "tag_assignments_suggested": len(tag_assignments),
        "health_alerts": len(health_alerts),
        "redundant_groups": len(redundant_groups),
        "vault_health_score": health_analyzer.compute_vault_health_score() if health_analyzer else 0,
        "execution_time_seconds": total_time,
    }

    if verbose:
        print("\n" + "=" * 50)
        print("‚úÖ ANALYSE TERMIN√âE")
        print("=" * 50)
        print(f"üìù Notes analys√©es: {stats['notes_analyzed']}")
        print(f"üè∑Ô∏è  Tags existants: {stats['existing_tags']}")
        print(f"üì¶ Clusters d√©tect√©s: {stats['clusters_detected']}")
        print(f"‚ú® Nouveaux tags sugg√©r√©s: {stats['new_tags_suggested']}")
        print(f"üìé Attributions sugg√©r√©es: {stats['tag_assignments_suggested']}")
        print(f"üîÑ Doublons d√©tect√©s: {stats['redundant_groups']}")
        print(f"‚ö†Ô∏è  Alertes de sant√©: {stats['health_alerts']}")
        print(f"üíö Score de sant√©: {stats['vault_health_score']:.0%}")
        print(f"‚è±Ô∏è  Temps total: {total_time:.1f}s")
        print("=" * 50)

    return stats


# ===== Adaptateurs pour la nouvelle architecture =====

class TagGeneratorV2:
    """G√©n√©rateur de tags adapt√© pour SimilarityEngineV2.

    Combine deux sources de suggestions :
    1. Clusters s√©mantiques de notes similaires
    2. Entit√©s nomm√©es d√©tect√©es dans le contenu (personnes, lieux, dates, etc.)
    """

    # Seuils
    MIN_ENTITY_NOTES = 3  # Nombre min de notes pour une suggestion d'entit√©
    MIN_CONFIDENCE_ENTITY = 0.60

    # Noms √† exclure des suggestions (faux positifs connus)
    EXCLUDED_ENTITY_NAMES = {
        "pasted-image", "pasted image", "image", "screenshot",
        "untitled", "sans titre", "new note", "nouvelle note",
        "test", "teste", "template", "mod√®le",
    }

    # Mots trop g√©n√©riques pour √™tre des tags utiles seuls
    # Ces mots sont utiles comme partie d'un tag compos√© (ex: "int√©grale\riemann")
    # mais pas seuls (ex: "int√©grale")
    TOO_GENERIC_WORDS = {
        # Mots math√©matiques tr√®s courants mais trop vagues seuls
        "produit", "somme", "mesure", "suite", "s√©rie", "serie",
        "limite", "fonction", "espace", "groupe", "corps", "module",
        "norme", "forme", "base", "point", "courbe", "surface",
        "vari√©t√©", "variete", "d√©riv√©e", "derivee", "m√©trique", "metrique",
        "th√©or√®me", "theoreme", "transformation", "int√©grale", "integrale",
        # Mots g√©n√©raux
        "exemple", "note", "chapitre", "section", "partie", "article",
        "question", "r√©ponse", "probl√®me", "solution", "m√©thode",
        # Si√®cles seuls (pr√©f√©rer avec p√©riode ou ann√©e)
        "i", "ii", "iii", "iv", "v", "vi", "vii", "viii", "ix", "x",
        "xi", "xii", "xiii", "xiv", "xv", "xvi", "xvii", "xviii", "xix", "xx", "xxi",
        # Ann√©es rondes trop g√©n√©riques
        "1000", "1100", "1200", "1300", "1400", "1500", "1600", "1700", "1800", "1900", "2000",
    }

    def __init__(
        self,
        clusters: list,
        similarity_engine: SimilarityEngineV2,
        existing_tags: list[str],
        repository: Repository,
        notes: Optional[list] = None,
    ):
        self.clusters = clusters
        self.engine = similarity_engine
        self.existing_tags = set(existing_tags)
        self.repository = repository
        self.notes = notes or []

        # D√©tecteur d'entit√©s V2 avec bases de r√©f√©rence
        self.reference_db = ReferenceDatabase()
        self.entity_detector = EntityDetectorV2(self.reference_db)

    def generate_suggestions(self, max_suggestions: int = 50) -> list[dict]:
        """G√©n√®re des suggestions de nouveaux tags.

        Combine suggestions de clusters et d'entit√©s d√©tect√©es.
        """
        suggestions = []

        # Tags rejet√©s pr√©c√©demment
        rejected_tags = self.repository.get_rejected_tag_names()

        # 1. Suggestions bas√©es sur les clusters
        cluster_suggestions = self._generate_cluster_suggestions(rejected_tags, max_suggestions)
        suggestions.extend(cluster_suggestions)

        # 2. Suggestions bas√©es sur les entit√©s d√©tect√©es
        if self.notes:
            entity_suggestions = self._generate_entity_suggestions(rejected_tags, max_suggestions)
            suggestions.extend(entity_suggestions)

        # D√©duplique et trie par confiance
        suggestions = self._deduplicate_suggestions(suggestions)
        suggestions.sort(key=lambda x: x["confidence"], reverse=True)

        return suggestions[:max_suggestions]

    def _generate_cluster_suggestions(
        self, rejected_tags: set, max_suggestions: int
    ) -> list[dict]:
        """G√©n√®re des suggestions bas√©es sur les clusters."""
        suggestions = []

        for cluster in self.clusters[:max_suggestions * 2]:
            # V√©rifie si le cluster a d√©j√† un tag appropri√©
            if cluster.suggested_tags:
                continue

            # G√©n√®re un nom de tag bas√© sur les termes du cluster
            if not cluster.centroid_terms:
                continue

            # Cr√©e un nom de tag
            tag_name = self._generate_tag_name(cluster)
            if not tag_name:
                continue

            # V√©rifie que le tag n'existe pas et n'a pas √©t√© rejet√©
            if tag_name in self.existing_tags or tag_name in rejected_tags:
                continue

            # Calcule la confiance
            confidence = min(0.95, cluster.coherence * 0.8 + 0.2 * (cluster.size / 20))

            suggestions.append({
                "id": f"lt_{len(suggestions):03d}",
                "name": tag_name,
                "confidence": confidence,
                "notes": cluster.notes[:10],
                "source": "cluster",
                "reasoning": {
                    "summary": f"Cluster de {cluster.size} notes avec termes communs: {', '.join(cluster.centroid_terms[:3])}",
                    "details": {
                        "cluster_size": cluster.size,
                        "coherence": cluster.coherence,
                        "centroid_terms": cluster.centroid_terms,
                    },
                },
            })

            if len(suggestions) >= max_suggestions:
                break

        return suggestions

    def _generate_entity_suggestions(
        self, rejected_tags: set, max_suggestions: int
    ) -> list[dict]:
        """G√©n√®re des suggestions bas√©es sur les entit√©s d√©tect√©es (V2 avec bases de r√©f√©rence)."""
        suggestions = []

        # D√©tecte les entit√©s dans toutes les notes avec le d√©tecteur V2
        notes_entities = self.entity_detector.detect_entities_batch(self.notes)

        # Agr√®ge les entit√©s qui apparaissent dans plusieurs notes
        aggregated = aggregate_entities_v2(
            notes_entities, min_notes=self.MIN_ENTITY_NOTES
        )

        # Collecte les infos d√©taill√©es sur chaque entit√©
        entity_details: dict = {}
        for path, note_entities in notes_entities.items():
            for entity in note_entities.entities:
                tag = entity.suggested_tag
                if tag in aggregated:
                    if tag not in entity_details:
                        entity_details[tag] = []
                    entity_details[tag].append(entity)

        for tag, note_paths in aggregated.items():
            # V√©rifie la redondance avec les tags existants
            if tag in self.existing_tags or tag in rejected_tags:
                continue

            # Filtre les faux positifs connus
            tag_lower = tag.lower()
            tag_base = tag_lower.replace("\\", " ").replace("-", " ").strip()
            if any(excluded in tag_base for excluded in self.EXCLUDED_ENTITY_NAMES):
                continue

            # Filtre les mots trop g√©n√©riques (sauf s'ils ont un pr√©fixe de convention)
            if "\\" not in tag and tag_lower in self.TOO_GENERIC_WORDS:
                continue

            # V√©rifie similarit√© avec tags existants (normalise)
            is_redundant = False
            for existing in self.existing_tags:
                existing_lower = existing.lower()
                # Similarit√© simple : contient ou est contenu
                if tag_lower in existing_lower or existing_lower in tag_lower:
                    is_redundant = True
                    break
            if is_redundant:
                continue

            # Calcule la confiance moyenne des d√©tections
            entities = entity_details.get(tag, [])
            if not entities:
                continue

            # === NOUVEAU: Score de confiance am√©lior√© V2 ===
            avg_detection_confidence = sum(e.confidence for e in entities) / len(entities)

            # Facteurs de confiance :
            # 1. Nombre de notes (3-10+) : 20% du score
            notes_count = len(note_paths)
            notes_factor = min(1.0, (notes_count - 2) / 7)  # 3 notes = 0.14, 10 notes = 1.0

            # 2. Occurrences totales : 15% du score
            total_occurrences = sum(e.occurrences for e in entities)
            occurrences_factor = min(1.0, total_occurrences / 15)

            # 3. Qualit√© de d√©tection (d√©j√† calibr√©e par le classificateur) : 40% du score
            detection_factor = min(1.0, (avg_detection_confidence - 0.3) / 0.6)

            # 4. NOUVEAU: Pr√©sence dans base de r√©f√©rence : 15% du score
            in_reference_db = any(e.source == "reference_db" for e in entities)
            reference_factor = 1.0 if in_reference_db else 0.3

            # 5. NOUVEAU: Pr√©sence dans titre : 10% du score
            in_title = any(getattr(e, 'in_title', False) for e in entities)
            title_factor = 1.0 if in_title else 0.5

            # Score final compos√©
            confidence = (
                0.40 * detection_factor +      # Qualit√© de d√©tection
                0.20 * notes_factor +           # Nombre de notes
                0.15 * occurrences_factor +     # Fr√©quence
                0.15 * reference_factor +       # Base de r√©f√©rence
                0.10 * title_factor             # Pr√©sence dans titre
            )

            # Ajuste l'√©chelle pour avoir des scores entre 0.50 et 0.95
            confidence = 0.50 + (confidence * 0.45)
            confidence = round(min(0.95, confidence), 2)

            if confidence < self.MIN_CONFIDENCE_ENTITY:
                continue

            # Construit le raisonnement
            entity_sample = entities[0]
            family_label = self._get_entity_type_label(entity_sample.entity_type)

            # Calcule les alternatives et v√©rifications de convention
            alternatives = self._compute_tag_alternatives(tag, entity_sample)

            suggestions.append({
                "id": f"lt_ent_{len(suggestions):03d}",
                "name": tag,
                "confidence": round(confidence, 2),
                "notes": note_paths[:10],
                "source": entity_sample.source,
                "alternatives": alternatives,
                "reasoning": {
                    "summary": f"{family_label} d√©tect√©(e) dans {len(note_paths)} notes",
                    "details": {
                        "entity_type": entity_sample.entity_type.value,
                        "raw_text": entity_sample.raw_text,
                        "total_occurrences": total_occurrences,
                        "notes_count": len(note_paths),
                        "in_reference_db": in_reference_db,
                        "in_title": in_title,
                    },
                },
            })

            if len(suggestions) >= max_suggestions:
                break

        return suggestions

    def _get_family_label(self, family: TagFamily) -> str:
        """Retourne un label lisible pour une famille de tags (V1 legacy)."""
        labels = {
            TagFamily.PERSON: "Personne",
            TagFamily.GEO: "Lieu g√©ographique",
            TagFamily.ENTITY: "Entit√© politique",
            TagFamily.AREA: "Aire culturelle",
            TagFamily.DATE: "Date/Si√®cle",
            TagFamily.CONCEPT_AUTHOR: "Concept/Auteur",
            TagFamily.DISCIPLINE: "Discipline",
            TagFamily.MATH_OBJECT: "Objet math√©matique",
            TagFamily.ARTWORK: "Mouvement artistique",
            TagFamily.CATEGORY: "Cat√©gorie",
            TagFamily.GENERIC: "Concept",
        }
        return labels.get(family, "Entit√©")

    def _get_entity_type_label(self, entity_type: EntityType) -> str:
        """Retourne un label lisible pour un type d'entit√© V2."""
        labels = {
            EntityType.PERSON: "Personne",
            EntityType.PLACE: "Lieu g√©ographique",
            EntityType.POLITICAL_ENTITY: "Entit√© politique",
            EntityType.DISCIPLINE: "Discipline",
            EntityType.CONCEPT: "Concept th√©orique",
            EntityType.ART_MOVEMENT: "Mouvement artistique",
            EntityType.DATE: "Date/Si√®cle",
            EntityType.UNKNOWN: "Entit√©",
        }
        return labels.get(entity_type, "Entit√©")

    def _compute_tag_alternatives(self, suggested_tag: str, entity) -> list[dict]:
        """Calcule les alternatives pour un tag sugg√©r√© (V2).

        Retourne une liste d'alternatives avec:
        - name: le nom alternatif du tag
        - reason: raison de l'alternative
        - is_convention: si c'est une correction de convention
        """
        alternatives = []

        # 1. V√©rifie si le tag respecte les conventions
        tag_info = classify_tag(suggested_tag)

        # Si le tag est g√©n√©rique mais pourrait avoir une convention sp√©cifique
        if tag_info.family == TagFamily.GENERIC:
            # Sugg√®re un format plus structur√© selon le type d'entit√© d√©tect√©e (V2)
            entity_type_to_family = {
                EntityType.PERSON: TagFamily.PERSON,
                EntityType.PLACE: TagFamily.GEO,
                EntityType.POLITICAL_ENTITY: TagFamily.ENTITY,
                EntityType.DISCIPLINE: TagFamily.DISCIPLINE,
                EntityType.DATE: TagFamily.DATE,
                EntityType.ART_MOVEMENT: TagFamily.ARTWORK,
            }
            expected_family = entity_type_to_family.get(entity.entity_type)
            if expected_family:
                # G√©n√®re le format conventionnel
                context = entity.metadata if hasattr(entity, 'metadata') else {}
                conventional = suggest_tag_format(
                    entity.raw_text, expected_family, context
                )
                if conventional != suggested_tag and conventional.lower() != suggested_tag.lower():
                    alternatives.append({
                        "name": conventional,
                        "reason": f"Format conventionnel pour {get_tag_family_label(expected_family)}",
                        "is_convention": True,
                    })

        # 2. Cherche des tags existants similaires
        similar_existing = self._find_similar_existing_tags(suggested_tag)
        for existing_tag, similarity_reason in similar_existing:
            alternatives.append({
                "name": existing_tag,
                "reason": similarity_reason,
                "is_convention": False,
            })

        return alternatives[:3]  # Limite √† 3 alternatives

    def _find_similar_existing_tags(self, tag: str) -> list[tuple[str, str]]:
        """Trouve des tags existants similaires au tag sugg√©r√©.

        Retourne une liste de (tag_existant, raison).
        """
        similar = []
        tag_lower = tag.lower()
        tag_parts = set(tag_lower.replace("\\", " ").replace("-", " ").split())

        for existing in self.existing_tags:
            existing_lower = existing.lower()

            # 1. Contenance simple
            if tag_lower in existing_lower:
                similar.append((existing, f"Le tag existant '{existing}' contient ce concept"))
                continue
            if existing_lower in tag_lower:
                similar.append((existing, f"Ce concept est inclus dans le tag existant '{existing}'"))
                continue

            # 2. Mots en commun
            existing_parts = set(existing_lower.replace("\\", " ").replace("-", " ").split())
            common_parts = tag_parts & existing_parts
            if len(common_parts) >= 1 and len(common_parts) >= len(tag_parts) * 0.5:
                common_str = ", ".join(common_parts)
                similar.append((existing, f"Mots communs: {common_str}"))

        return similar[:2]  # Limite √† 2 tags similaires

    def _deduplicate_suggestions(self, suggestions: list[dict]) -> list[dict]:
        """D√©duplique les suggestions en gardant celle avec la meilleure confiance.

        Normalise les accents pour √©viter les doublons comme s√©rie/serie.
        """
        import unicodedata

        def normalize_key(name: str) -> str:
            """Normalise un nom en retirant les accents."""
            # D√©compose les caract√®res accentu√©s et retire les accents
            normalized = unicodedata.normalize('NFD', name.lower())
            without_accents = ''.join(c for c in normalized if unicodedata.category(c) != 'Mn')
            return without_accents

        seen: dict = {}
        for suggestion in suggestions:
            key = normalize_key(suggestion["name"])
            if key not in seen or suggestion["confidence"] > seen[key]["confidence"]:
                seen[key] = suggestion
        return list(seen.values())

    def _generate_tag_name(self, cluster) -> Optional[str]:
        """G√©n√®re un nom de tag √† partir des termes du cluster."""
        if not cluster.centroid_terms:
            return None

        # Prend les 2-3 premiers termes
        terms = cluster.centroid_terms[:2]

        # Capitalise et joint
        name = "/".join(t.capitalize() for t in terms)

        # Nettoie
        name = name.replace(" ", "-")

        return name if len(name) > 2 else None


class TagMatcherV2:
    """Matcher de tags adapt√© pour SimilarityEngineV2."""

    def __init__(
        self,
        notes_dict: dict,
        similarity_engine: SimilarityEngineV2,
        existing_tags: list[str],
        repository: Repository,
    ):
        self.notes = notes_dict
        self.engine = similarity_engine
        self.existing_tags = existing_tags
        self.repository = repository

        # Index tags par note
        self._notes_by_tag: dict[str, list[str]] = {}
        for path, note in notes_dict.items():
            for tag in note.tags:
                if tag not in self._notes_by_tag:
                    self._notes_by_tag[tag] = []
                self._notes_by_tag[tag].append(path)

    def find_suggestions(self, max_suggestions: int = 100) -> list[dict]:
        """Trouve des suggestions d'attribution de tags existants."""
        suggestions = []

        # Tags populaires (utilises par au moins 3 notes)
        popular_tags = {tag for tag, notes in self._notes_by_tag.items() if len(notes) >= 3}

        # Pour chaque note sans beaucoup de tags
        for path, note in self.notes.items():
            if len(note.tags) >= 8:  # Skip si deja tres bien tague
                continue

            # Trouve les voisins avec un seuil plus bas
            neighbors = self.engine.find_neighbors(path, k=15, threshold=0.55)

            # Collecte les tags des voisins
            neighbor_tags: dict[str, list[tuple[str, float]]] = {}
            for neighbor_path, score in neighbors.neighbors:
                neighbor_note = self.notes.get(neighbor_path)
                if neighbor_note:
                    for tag in neighbor_note.tags:
                        # Tag que la note n'a pas et qui est populaire
                        if tag not in note.tags and tag in popular_tags:
                            if tag not in neighbor_tags:
                                neighbor_tags[tag] = []
                            neighbor_tags[tag].append((neighbor_path, score))

            # Suggere les tags les plus frequents chez les voisins
            for tag, sources in neighbor_tags.items():
                # Au moins 1 voisin avec score > 0.7, ou 2+ voisins
                high_score_sources = [s for s in sources if s[1] > 0.7]
                if len(sources) < 2 and len(high_score_sources) < 1:
                    continue

                # Verifie si suggestion existe deja
                if self.repository.suggestion_exists(tag, path):
                    continue

                avg_score = sum(s[1] for s in sources) / len(sources)
                # Confiance basee sur le score et le nombre de sources
                confidence = min(0.95, avg_score * 0.7 + 0.3 * min(1.0, len(sources) / 3))

                if confidence < 0.45:
                    continue

                suggestions.append({
                    "id": f"ta_{len(suggestions):03d}",
                    "note": path,
                    "tag": tag,
                    "confidence": confidence,
                    "reasoning": {
                        "summary": f"{len(sources)} notes similaires ont ce tag",
                        "details": {
                            "matching_notes": [
                                {"path": s[0], "similarity": s[1]}
                                for s in sources[:3]
                            ],
                        },
                    },
                })

                if len(suggestions) >= max_suggestions:
                    return suggestions

        # Trie par confiance
        suggestions.sort(key=lambda x: x["confidence"], reverse=True)
        return suggestions[:max_suggestions]


if __name__ == "__main__":
    cli()
